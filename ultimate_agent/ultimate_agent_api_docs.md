# üöÄ Ultimate Agent API Documentation v4.1

## Overview

The Enhanced Ultimate Pain Network Agent provides a comprehensive REST API and WebSocket interface for managing AI training, blockchain operations, task execution, system monitoring, conversational AI, and P2P distributed inference. This API follows RESTful principles and supports both JSON and real-time communication.

**Base URL:** `http://localhost:8080/api`
**API Version:** v4.1 (latest), with backward compatibility for v4, v3, and v1
**Content-Type:** `application/json`

## ‚ú® What's New in v4.1

- ü§ñ **Conversational AI Chat Interface** - Full-featured AI assistant with context memory
- üåê **P2P Distributed Inference** - Decentralized AI processing across network nodes
- üîÑ **Advanced Ollama Integration** - Enhanced local AI model management with streaming
- üí¨ **Real-time Chat WebSocket** - Live conversation capabilities
- üß† **Multi-Model AI Pipeline** - Sequential and parallel model execution
- üìä **Enhanced Monitoring** - Comprehensive system health and performance tracking

---

## üìö Table of Contents

1. [Authentication](#authentication)
2. [Agent Status & Info](#agent-status--info)
3. [Task Management](#task-management)
4. [AI Operations](#ai-operations)
5. [**NEW** Conversational AI Chat](#conversational-ai-chat)
6. [**NEW** P2P Distributed Inference](#p2p-distributed-inference)
7. [**ENHANCED** Advanced AI Backends](#advanced-ai-backends)
8. [Blockchain Operations](#blockchain-operations)
9. [System Monitoring](#system-monitoring)
10. [Remote Management](#remote-management)
11. [Database Operations](#database-operations)
12. [**ENHANCED** WebSocket Events](#websocket-events)
13. [Error Handling](#error-handling)
14. [Rate Limiting](#rate-limiting)
15. [**NEW** Integration Examples](#integration-examples)

---

## üîê Authentication

### API Key Authentication
```http
Authorization: Bearer <api_key>
```

### Generate API Token
```http
POST /api/auth/token
Content-Type: application/json

{
  "agent_id": "agent-123",
  "permissions": ["read", "write", "admin", "chat", "p2p"]
}
```

**Response:**
```json
{
  "token": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...",
  "expires_in": 3600,
  "permissions": ["read", "write", "admin", "chat", "p2p"],
  "features_enabled": ["conversational_ai", "p2p_inference", "advanced_ollama"]
}
```

---

## üìä Agent Status & Info

### Get Enhanced Agent Status
```http
GET /api/v4/stats/enhanced
```

**Response:**
```json
{
  "agent_id": "ultimate-a1b2c3d4",
  "running": true,
  "registered": true,
  "current_tasks": 2,
  "stats": {
    "tasks_completed": 157,
    "tasks_failed": 3,
    "total_earnings": 0.2847,
    "start_time": 1640995200,
    "conversations_active": 3,
    "p2p_nodes_connected": 7
  },
  "ai_status": {
    "models_loaded": 12,
    "gpu_available": true,
    "training_engine_active": true,
    "active_training_sessions": 1,
    "conversation_manager_active": true,
    "ollama_instances": 2,
    "distributed_inference_enabled": true
  },
  "blockchain_status": {
    "wallet_initialized": true,
    "smart_contracts_enabled": true,
    "connected_networks": 3,
    "transaction_pool_size": 5
  },
  "p2p_status": {
    "network_enabled": true,
    "connected_peers": 7,
    "known_nodes": 15,
    "distributed_models": 8,
    "consensus_active": true
  },
  "modular_architecture": true,
  "advanced_features": true,
  "api_version": "4.1"
}
```

### Get Agent Capabilities
```http
GET /api/v4/capabilities
```

**Response:**
```json
{
  "agent_id": "ultimate-a1b2c3d4",
  "name": "ultimate-agent-a1b2c3d4",
  "version": "4.1.0-enhanced",
  "agent_type": "ultimate",
  "capabilities": [
    "ai", "blockchain", "cloud", "security", 
    "conversational_ai", "p2p_inference", "distributed_training"
  ],
  "ai_models": [
    "sentiment", "classification", "regression", "transformer", 
    "cnn", "llama2", "codellama", "mistral"
  ],
  "chat_models": [
    "general", "technical", "creative", "sentiment", "transformer"
  ],
  "p2p_capabilities": [
    "distributed_inference", "model_sharding", "consensus_validation", 
    "load_balancing", "fault_tolerance"
  ],
  "ollama_features": [
    "streaming_inference", "multi_instance", "load_balancing", 
    "health_monitoring", "model_lifecycle"
  ],
  "task_types": [
    "neural_network_training", "transformer_training", "cnn_training",
    "distributed_inference", "federated_learning", "conversation_task",
    "blockchain_transaction", "smart_contract_execution"
  ],
  "enhanced_features": {
    "real_time_chat": true,
    "context_memory": true,
    "voice_support": true,
    "file_analysis": true,
    "multi_model_pipeline": true,
    "p2p_coordination": true
  }
}
```

---

## ü§ñ Conversational AI Chat

### Start New Conversation
```http
POST /api/ai/conversations
Content-Type: application/json

{
  "user_id": "local_user",
  "model_type": "general",
  "conversation_title": "General Discussion"
}
```

**Response:**
```json
{
  "success": true,
  "conversation_id": "conv_1640995400abc",
  "model_type": "general",
  "created_at": "2023-12-31T17:00:00Z",
  "welcome_message": "Hello! I'm your personal AI assistant. How can I help you today?"
}
```

### Send Chat Message
```http
POST /api/ai/chat
Content-Type: application/json

{
  "input": "Explain quantum computing in simple terms",
  "conversation_id": "conv_1640995400abc",
  "model": "technical",
  "options": {
    "context_aware": true,
    "return_confidence": true,
    "max_tokens": 500
  }
}
```

**Response:**
```json
{
  "success": true,
  "conversation_id": "conv_1640995400abc",
  "response": "Quantum computing is like having a supercomputer that can explore multiple solutions simultaneously...",
  "confidence": 0.94,
  "response_time": 1.25,
  "message_count": 3,
  "context_used": true,
  "strategy": "technical",
  "model_used": "technical",
  "tokens_used": 87
}
```

### Get Conversation History
```http
GET /api/ai/conversations/conv_1640995400abc
```

**Response:**
```json
{
  "success": true,
  "conversation": {
    "id": "conv_1640995400abc",
    "user_id": "local_user",
    "model_type": "technical",
    "created_at": "2023-12-31T17:00:00Z",
    "last_activity": "2023-12-31T17:05:00Z",
    "messages": [
      {
        "role": "user",
        "content": "Explain quantum computing in simple terms",
        "timestamp": "2023-12-31T17:00:00Z",
        "id": "msg_abc123"
      },
      {
        "role": "assistant", 
        "content": "Quantum computing is like having a supercomputer...",
        "timestamp": "2023-12-31T17:00:05Z",
        "id": "msg_def456"
      }
    ],
    "metadata": {
      "message_count": 2,
      "total_tokens": 154,
      "avg_response_time": 1.25
    }
  }
}
```

### List User Conversations
```http
GET /api/ai/conversations?user_id=local_user&limit=20
```

**Response:**
```json
{
  "success": true,
  "conversations": [
    {
      "id": "conv_1640995400abc",
      "created_at": "2023-12-31T17:00:00Z",
      "last_activity": "2023-12-31T17:05:00Z",
      "message_count": 5,
      "model_type": "technical",
      "last_message_preview": "That's a great explanation of quantum entanglement..."
    }
  ],
  "total_count": 8,
  "page": 1,
  "limit": 20
}
```

### Chat Statistics
```http
GET /api/ai/chat/statistics
```

**Response:**
```json
{
  "success": true,
  "statistics": {
    "total_conversations": 8,
    "total_messages": 42,
    "active_conversations": 2,
    "model_usage": {
      "general": 3,
      "technical": 2,
      "creative": 2,
      "sentiment": 1
    },
    "average_response_time": 1.15,
    "chat_history_size": 156,
    "context_memory_enabled": true
  }
}
```

### Available Chat Models
- `general` - General-purpose conversational AI
- `technical` - Technical and programming assistance
- `creative` - Creative writing and brainstorming
- `sentiment` - Emotion-aware conversation
- `transformer` - Advanced language understanding

---

## üåê P2P Distributed Inference

### Get P2P Network Status
```http
GET /api/v4/p2p/status
```

**Response:**
```json
{
  "enabled": true,
  "running": true,
  "node_id": "ultimate-a1b2c3d4",
  "node_type": "full_node",
  "connected_peers": 7,
  "known_nodes": 15,
  "local_shards": 3,
  "active_inferences": 2,
  "network_health": 0.87,
  "metrics": {
    "messages_sent": 245,
    "messages_received": 198,
    "inferences_completed": 23,
    "consensus_reached": 21,
    "average_latency": 145.7
  },
  "capabilities": {
    "models": ["sentiment", "classification", "transformer"],
    "compute_power": 2.5e9,
    "memory_gb": 16.0,
    "gpu_available": true,
    "bandwidth_mbps": 100.0
  }
}
```

### Start P2P Network
```http
POST /api/v4/p2p/start
Content-Type: application/json

{
  "bootstrap_nodes": [
    "ultimate-node1.example.com:4001",
    "ultimate-node2.example.com:4001"
  ],
  "node_type": "full_node"
}
```

**Response:**
```json
{
  "success": true,
  "message": "P2P network started",
  "node_id": "ultimate-a1b2c3d4",
  "listen_port": 4001,
  "connected_peers": 2,
  "bootstrap_success": true
}
```

### Execute Distributed Inference
```http
POST /api/v4/p2p/inference
Content-Type: application/json

{
  "model": "transformer",
  "input": "Translate this text to French: Hello, how are you?",
  "options": {
    "priority": 8,
    "timeout": 30.0,
    "redundancy": 3,
    "consensus_required": true
  }
}
```

**Response:**
```json
{
  "success": true,
  "result": "Salut, comment allez-vous?",
  "consensus_reached": true,
  "nodes_used": 3,
  "execution_time": 2.45,
  "confidence": 0.96,
  "distributed_metrics": {
    "coordination_time": 0.25,
    "inference_time": 1.85,
    "consensus_time": 0.35,
    "participating_nodes": [
      "ultimate-node1", "ultimate-node2", "ultimate-node3"
    ],
    "consensus_agreement": 0.98
  }
}
```

### Get Connected Peers
```http
GET /api/v4/p2p/peers
```

**Response:**
```json
{
  "peers": 7,
  "known_nodes": 15,
  "node_id": "ultimate-a1b2c3d4",
  "peer_details": [
    {
      "node_id": "ultimate-node1",
      "node_type": "compute_node",
      "connected_at": "2023-12-31T16:30:00Z",
      "last_heartbeat": "2023-12-31T17:04:00Z",
      "models": ["sentiment", "classification"],
      "compute_power": 1.8e9,
      "latency_ms": 125.3,
      "reliability_score": 0.95
    }
  ]
}
```

### Get Available Distributed Models
```http
GET /api/v4/p2p/models
```

**Response:**
```json
{
  "local_models": ["sentiment", "classification", "transformer"],
  "network_models": [
    {
      "model_id": "llama2-70b",
      "available_nodes": 5,
      "shard_count": 8,
      "total_parameters": "70B",
      "estimated_latency": 3.2,
      "consensus_support": true
    },
    {
      "model_id": "codellama-34b", 
      "available_nodes": 3,
      "shard_count": 4,
      "total_parameters": "34B",
      "estimated_latency": 2.1,
      "consensus_support": true
    }
  ],
  "model_registry": {
    "total_unique_models": 12,
    "sharded_models": 4,
    "replicated_models": 8
  }
}
```

---

## üß† Advanced AI Backends

### Get Ollama Status
```http
GET /api/v4/ai/ollama/status
```

**Response:**
```json
{
  "instances_available": 2,
  "active_connections": 2,
  "cloud_compute_instances": 0,
  "total_operations": 156,
  "instance_details": [
    {
      "instance_id": "localhost:11434",
      "status": "healthy",
      "health_score": 0.95,
      "active_connections": 3,
      "total_requests": 89,
      "failed_requests": 2,
      "avg_response_time": 1.25,
      "available_models": ["llama2", "codellama", "mistral"],
      "last_health_check": "2023-12-31T17:04:30Z"
    }
  ],
  "load_balancing": {
    "strategy": "response_time",
    "total_requests": 156,
    "successful_requests": 152,
    "success_rate": 0.974
  }
}
```

### Stream Ollama Inference
```http
POST /api/v4/ai/ollama/generate/stream
Content-Type: application/json

{
  "model": "llama2",
  "prompt": "Write a Python function to calculate fibonacci numbers",
  "stream": true,
  "options": {
    "temperature": 0.7,
    "max_tokens": 500
  }
}
```

**Response (Server-Sent Events):**
```
data: {"response": "Here's a Python function", "done": false}

data: {"response": " to calculate fibonacci numbers:\n\n", "done": false}

data: {"response": "```python\ndef fibonacci(n):", "done": false}

data: {"response": "\n    if n <= 1:\n        return n", "done": false}

data: {"response": "\n    return fibonacci(n-1) + fibonacci(n-2)\n```", "done": true, "total_duration": 2450000000}
```

### Manage Ollama Models
```http
POST /api/v4/ai/ollama/models/pull
Content-Type: application/json

{
  "model": "codellama:7b",
  "instance_id": "localhost:11434"
}
```

**Response:**
```json
{
  "success": true,
  "model": "codellama:7b",
  "instance_results": {
    "localhost:11434": true
  },
  "download_progress": {
    "localhost:11434:codellama:7b": 100.0
  }
}
```

### Get AI Pipeline Status
```http
GET /api/v4/ai/pipeline/status
```

**Response:**
```json
{
  "active_pipelines": 2,
  "pipeline_types": ["sequential", "parallel", "distributed"],
  "processing_queue": 3,
  "completed_pipelines": 45,
  "average_processing_time": 3.2,
  "supported_backends": [
    "local_inference", "ollama", "p2p_distributed", "cloud_gpu"
  ]
}
```

---

## üéØ Enhanced Task Management

### Start Advanced AI Task
```http
POST /api/v4/tasks/start
Content-Type: application/json

{
  "type": "distributed_transformer_training",
  "config": {
    "model_type": "transformer",
    "layers": 12,
    "attention_heads": 8,
    "hidden_size": 768,
    "epochs": 5,
    "distributed": true,
    "p2p_nodes": 3,
    "consensus_training": true
  },
  "priority": 8,
  "resource_requirements": {
    "gpu_memory_gb": 8,
    "system_memory_gb": 16,
    "network_bandwidth_mbps": 100
  }
}
```

**Response:**
```json
{
  "success": true,
  "task_id": "task-dist-1640995500-abcd",
  "task_type": "distributed_transformer_training",
  "estimated_duration": 450,
  "status": "initializing",
  "distributed_setup": {
    "coordinator_node": "ultimate-a1b2c3d4",
    "participant_nodes": ["ultimate-node1", "ultimate-node2"],
    "model_shards": 3,
    "consensus_enabled": true
  }
}
```

### Get Advanced Task Status
```http
GET /api/v4/tasks/task-dist-1640995500-abcd/status
```

**Response:**
```json
{
  "task_id": "task-dist-1640995500-abcd",
  "status": "training",
  "progress": 34.5,
  "current_stage": "epoch_2_attention_layers",
  "distributed_metrics": {
    "nodes_active": 3,
    "synchronization_lag_ms": 45.2,
    "consensus_agreement": 0.97,
    "gradient_sync_frequency": "every_100_steps"
  },
  "training_metrics": {
    "current_epoch": 2,
    "total_epochs": 5,
    "training_loss": 2.34,
    "validation_loss": 2.28,
    "perplexity": 10.4,
    "tokens_per_second": 1250.5
  },
  "resource_usage": {
    "gpu_memory_used_gb": 7.2,
    "network_bandwidth_mbps": 87.5,
    "estimated_completion": "2023-12-31T17:15:00Z"
  }
}
```

### Enhanced Task Types
- `distributed_transformer_training` - Distributed transformer training across P2P network
- `federated_learning_round` - Federated learning coordination
- `consensus_inference` - Multi-node consensus-based inference
- `model_sharding_deployment` - Deploy sharded models across network
- `conversation_training` - Train conversational AI models
- `multi_modal_pipeline` - Sequential multi-model processing
- `streaming_inference_pipeline` - Real-time streaming inference

---

## üí∞ Enhanced Blockchain Operations

### Execute Advanced Smart Contract
```http
POST /api/v4/blockchain/contracts/execute
Content-Type: application/json

{
  "contract_type": "ai_marketplace",
  "method": "purchaseModel",
  "params": {
    "model_id": "transformer-7b-fine-tuned",
    "price": 2.5,
    "license_type": "commercial",
    "usage_terms": {
      "max_inferences": 10000,
      "geographic_restrictions": ["US", "EU"],
      "commercial_use": true
    }
  },
  "gas_limit": 150000,
  "consensus_required": true
}
```

**Response:**
```json
{
  "success": true,
  "transaction_hash": "0x9f8e7d6c5b4a3918273645567890abcdef1234567890abcdef1234567890abcd",
  "gas_used": 87542,
  "block_number": 18750001,
  "contract_address": "0x1234567890abcdef1234567890abcdef12345678",
  "execution_result": {
    "model_id": "transformer-7b-fine-tuned",
    "purchase_price": 2.5,
    "buyer": "0xa1b2c3d4e5f6789012345678901234567890abcd",
    "license_type": "commercial",
    "purchased_at": 1640995500,
    "license_token_id": "nft_license_789456"
  },
  "consensus_validation": {
    "validators": 5,
    "consensus_reached": true,
    "validation_time_ms": 245
  }
}
```

### Get Enhanced Blockchain Metrics
```http
GET /api/v4/blockchain/metrics
```

**Response:**
```json
{
  "wallet_stats": {
    "total_value_usd": 1247.89,
    "earnings_this_session": 0.157,
    "total_transactions": 89,
    "gas_spent_total": 0.0234
  },
  "smart_contract_usage": {
    "contracts_deployed": 5,
    "total_executions": 234,
    "execution_success_rate": 0.987,
    "most_used_contract": "task_rewards"
  },
  "network_participation": {
    "consensus_validations": 156,
    "block_proposals": 12,
    "network_reputation": 0.92
  },
  "defi_integration": {
    "liquidity_provided": 45.7,
    "yield_farming_active": true,
    "staking_rewards_earned": 2.34
  }
}
```

---

## üìä Enhanced System Monitoring

### Get Comprehensive Health Status
```http
GET /api/v4/health/comprehensive
```

**Response:**
```json
{
  "overall_health": "excellent",
  "health_score": 94.2,
  "components": {
    "core_agent": {
      "status": "healthy",
      "score": 98.5,
      "uptime_hours": 72.3,
      "memory_usage_mb": 512.7
    },
    "ai_systems": {
      "status": "healthy", 
      "score": 95.2,
      "models_loaded": 12,
      "active_inferences": 3,
      "ollama_instances": 2,
      "inference_success_rate": 0.987
    },
    "p2p_network": {
      "status": "healthy",
      "score": 89.7,
      "connected_peers": 7,
      "network_latency_ms": 145.3,
      "consensus_success_rate": 0.956
    },
    "blockchain": {
      "status": "healthy",
      "score": 92.4,
      "transaction_success_rate": 0.994,
      "smart_contract_executions": 234,
      "network_sync_status": "synced"
    },
    "conversation_ai": {
      "status": "healthy",
      "score": 96.8,
      "active_conversations": 3,
      "average_response_time": 1.15,
      "context_memory_efficiency": 0.94
    }
  },
  "performance_metrics": {
    "cpu_efficiency": 0.892,
    "memory_efficiency": 0.874,
    "network_efficiency": 0.923,
    "task_completion_rate": 0.978
  },
  "recommendations": [
    "Consider increasing P2P connection pool for better redundancy",
    "Monitor GPU memory usage during peak inference periods"
  ]
}
```

### Get AI Performance Analytics
```http
GET /api/v4/analytics/ai-performance?timeframe=24h
```

**Response:**
```json
{
  "timeframe": "24h",
  "inference_analytics": {
    "total_inferences": 1247,
    "successful_inferences": 1235,
    "average_latency_ms": 156.7,
    "model_usage": {
      "sentiment": 345,
      "classification": 298,
      "transformer": 267,
      "llama2": 187,
      "codellama": 150
    },
    "performance_trends": {
      "latency_trend": "stable",
      "accuracy_trend": "improving",
      "throughput_trend": "increasing"
    }
  },
  "distributed_analytics": {
    "p2p_inferences": 89,
    "consensus_reached": 86,
    "average_consensus_time_ms": 234.5,
    "node_participation": {
      "local_node": 0.34,
      "remote_nodes": 0.66
    }
  },
  "conversation_analytics": {
    "total_messages": 156,
    "conversations_started": 12,
    "average_conversation_length": 13.2,
    "user_satisfaction_estimate": 0.89,
    "most_used_model": "general"
  }
}
```

---

## üåê Enhanced WebSocket Events

### Core Events

#### Connection Events
```javascript
socket.on('connected', (data) => {
    // Enhanced connection data
    console.log('Agent ID:', data.agent_id);
    console.log('Version:', data.version);
    console.log('Features:', data.enhanced_features);
    console.log('P2P Enabled:', data.p2p_enabled);
    console.log('Chat Enabled:', data.chat_enabled);
});
```

#### Real-time AI Events
```javascript
// Chat message handling
socket.on('chat_message', (data) => {
    console.log('User:', data.message);
    console.log('Model:', data.model);
});

socket.on('chat_response', (data) => {
    console.log('AI Response:', data.response);
    console.log('Confidence:', data.confidence);
    console.log('Response Time:', data.response_time);
});

// P2P Network Events  
socket.on('p2p_node_connected', (data) => {
    console.log('New P2P node:', data.node_id);
    console.log('Node type:', data.node_type);
    console.log('Capabilities:', data.capabilities);
});

socket.on('distributed_inference_started', (data) => {
    console.log('Distributed inference:', data.task_id);
    console.log('Participating nodes:', data.nodes);
});

socket.on('distributed_inference_completed', (data) => {
    console.log('Inference completed:', data.task_id);
    console.log('Consensus reached:', data.consensus_reached);
    console.log('Result:', data.result);
});

// Ollama Events
socket.on('ollama_model_loaded', (data) => {
    console.log('Model loaded:', data.model_name);
    console.log('Instance:', data.instance_id);
});

socket.on('ollama_streaming_chunk', (data) => {
    console.log('Streaming chunk:', data.chunk);
    console.log('Done:', data.done);
});
```

#### Enhanced Task Events
```javascript
socket.on('task_progress_detailed', (data) => {
    console.log('Task:', data.task_id);
    console.log('Progress:', data.progress);
    console.log('Stage:', data.current_stage);
    console.log('Distributed metrics:', data.distributed_metrics);
    console.log('Resource usage:', data.resource_usage);
});

socket.on('consensus_training_update', (data) => {
    console.log('Consensus training:', data.task_id);
    console.log('Nodes participating:', data.nodes_active);
    console.log('Synchronization lag:', data.sync_lag_ms);
    console.log('Agreement level:', data.consensus_agreement);
});
```

### WebSocket API Methods

#### Chat Operations
```javascript
// Start new conversation
socket.emit('start_conversation', {
    model_type: 'technical',
    user_id: 'user123'
});

// Send chat message
socket.emit('chat_message', {
    message: 'Explain machine learning',
    conversation_id: 'conv_abc123',
    model: 'technical'
});

// Request conversation history
socket.emit('get_conversation_history', {
    conversation_id: 'conv_abc123',
    limit: 50
});
```

#### P2P Operations
```javascript
// Request P2P status
socket.emit('get_p2p_status');

// Start distributed inference
socket.emit('start_distributed_inference', {
    model: 'transformer',
    input: 'Translate to French: Hello world',
    priority: 8,
    consensus_required: true
});

// Join P2P network
socket.emit('join_p2p_network', {
    bootstrap_nodes: ['node1.example.com:4001']
});
```

#### Advanced Monitoring
```javascript
// Request comprehensive health data
socket.emit('get_comprehensive_health');

// Subscribe to AI performance metrics
socket.emit('subscribe_ai_metrics', {
    interval_seconds: 5,
    include_distributed: true
});

// Request blockchain analytics
socket.emit('get_blockchain_analytics', {
    timeframe: '1h'
});
```

---

## üîó Integration Examples

### Full-Stack Chat Integration

#### Frontend (JavaScript)
```javascript
class UltimateAgentChatClient {
    constructor(baseUrl = 'http://localhost:8080') {
        this.baseUrl = baseUrl;
        this.socket = io(baseUrl);
        this.conversationId = null;
        this.setupEventHandlers();
    }
    
    setupEventHandlers() {
        this.socket.on('chat_response', (data) => {
            this.displayMessage('ai', data.response);
            this.updateMetrics(data);
        });
        
        this.socket.on('p2p_inference_result', (data) => {
            this.displayDistributedResult(data);
        });
    }
    
    async startConversation(modelType = 'general') {
        const response = await fetch(`${this.baseUrl}/api/ai/conversations`, {
            method: 'POST',
            headers: {'Content-Type': 'application/json'},
            body: JSON.stringify({
                user_id: 'user123',
                model_type: modelType
            })
        });
        
        const data = await response.json();
        this.conversationId = data.conversation_id;
        return data;
    }
    
    sendMessage(message, useDistributed = false) {
        if (useDistributed) {
            this.socket.emit('start_distributed_inference', {
                model: 'transformer',
                input: message,
                conversation_id: this.conversationId
            });
        } else {
            this.socket.emit('chat_message', {
                message,
                conversation_id: this.conversationId,
                model: 'general'
            });
        }
    }
    
    displayMessage(sender, content) {
        const messageDiv = document.createElement('div');
        messageDiv.className = `message ${sender}`;
        messageDiv.textContent = content;
        document.getElementById('chat-container').appendChild(messageDiv);
    }
    
    displayDistributedResult(data) {
        const resultDiv = document.createElement('div');
        resultDiv.className = 'distributed-result';
        resultDiv.innerHTML = `
            <div class="result">${data.result}</div>
            <div class="metrics">
                Nodes: ${data.nodes_used} | 
                Consensus: ${data.consensus_reached ? 'Yes' : 'No'} |
                Time: ${data.execution_time}s
            </div>
        `;
        document.getElementById('chat-container').appendChild(resultDiv);
    }
}

// Usage
const chatClient = new UltimateAgentChatClient();
chatClient.startConversation('technical').then(() => {
    chatClient.sendMessage('Explain neural networks');
});
```

#### Backend Integration (Python)
```python
import asyncio
import websockets
import json
from typing import Dict, Any, Optional

class UltimateAgentIntegration:
    def __init__(self, agent_url: str = "http://localhost:8080"):
        self.agent_url = agent_url
        self.session = aiohttp.ClientSession()
        self.websocket = None
        
    async def connect(self):
        """Connect to agent WebSocket"""
        ws_url = self.agent_url.replace('http', 'ws') + '/socket.io/'
        self.websocket = await websockets.connect(ws_url)
        
        # Setup event handlers
        asyncio.create_task(self._handle_websocket_messages())
        
    async def _handle_websocket_messages(self):
        """Handle incoming WebSocket messages"""
        async for message in self.websocket:
            data = json.loads(message)
            await self._process_websocket_event(data)
    
    async def _process_websocket_event(self, data: Dict[str, Any]):
        """Process WebSocket events"""
        event_type = data.get('type')
        
        if event_type == 'chat_response':
            await self._handle_chat_response(data['data'])
        elif event_type == 'distributed_inference_completed':
            await self._handle_distributed_result(data['data'])
        elif event_type == 'task_completed':
            await self._handle_task_completion(data['data'])
    
    async def start_distributed_training(self, config: Dict[str, Any]) -> str:
        """Start distributed AI training task"""
        async with self.session.post(
            f"{self.agent_url}/api/v4/tasks/start",
            json={
                "type": "distributed_transformer_training",
                "config": config,
                "priority": 8
            }
        ) as response:
            data = await response.json()
            return data['task_id']
    
    async def chat_with_ai(self, message: str, model: str = 'general',
                          use_distributed: bool = False) -> Dict[str, Any]:
        """Send chat message to AI"""
        endpoint = "/api/v4/p2p/inference" if use_distributed else "/api/ai/chat"
        
        payload = {
            "input" if use_distributed else "input": message,
            "model": model
        }
        
        if use_distributed:
            payload.update({
                "options": {
                    "priority": 7,
                    "consensus_required": True
                }
            })
        
        async with self.session.post(
            f"{self.agent_url}{endpoint}",
            json=payload
        ) as response:
            return await response.json()
    
    async def get_comprehensive_status(self) -> Dict[str, Any]:
        """Get comprehensive agent status"""
        async with self.session.get(
            f"{self.agent_url}/api/v4/stats/enhanced"
        ) as response:
            return await response.json()
    
    async def monitor_p2p_network(self) -> Dict[str, Any]:
        """Monitor P2P network status"""
        async with self.session.get(
            f"{self.agent_url}/api/v4/p2p/status"
        ) as response:
            return await response.json()
    
    async def close(self):
        """Close connections"""
        if self.websocket:
            await self.websocket.close()
        await self.session.close()

# Usage Example
async def main():
    agent = UltimateAgentIntegration()
    await agent.connect()
    
    # Check status
    status = await agent.get_comprehensive_status()
    print(f"Agent Status: {status['running']}")
    print(f"P2P Enabled: {status['p2p_status']['enabled']}")
    
    # Start distributed training
    task_id = await agent.start_distributed_training({
        "model_type": "transformer",
        "layers": 12,
        "distributed": True,
        "epochs": 5
    })
    print(f"Distributed training started: {task_id}")
    
    # Chat with AI
    response = await agent.chat_with_ai(
        "Explain distributed training",
        model="technical"
    )
    print(f"AI Response: {response['response']}")
    
    # Use distributed inference
    distributed_response = await agent.chat_with_ai(
        "What are the benefits of P2P AI networks?",
        model="technical",
        use_distributed=True
    )
    print(f"Distributed AI Response: {distributed_response['result']}")
    print(f"Consensus Reached: {distributed_response['consensus_reached']}")
    
    await agent.close()

if __name__ == "__main__":
    asyncio.run(main())
```

### Mobile SDK Integration (React Native)

```javascript
// UltimateAgentMobile.js
import io from 'socket.io-client';

class UltimateAgentMobile {
    constructor(agentUrl = 'http://192.168.1.100:8080') {
        this.agentUrl = agentUrl;
        this.socket = null;
        this.conversationId = null;
    }
    
    async initialize() {
        this.socket = io(this.agentUrl);
        
        return new Promise((resolve) => {
            this.socket.on('connected', (data) => {
                console.log('Connected to Ultimate Agent Mobile');
                resolve(data);
            });
        });
    }
    
    async startVoiceConversation() {
        // Start conversation with voice support
        const response = await fetch(`${this.agentUrl}/api/ai/conversations`, {
            method: 'POST',
            headers: {'Content-Type': 'application/json'},
            body: JSON.stringify({
                user_id: 'mobile_user',
                model_type: 'general',
                voice_enabled: true
            })
        });
        
        const data = await response.json();
        this.conversationId = data.conversation_id;
        return data;
    }
    
    sendVoiceMessage(audioBlob) {
        // Convert audio to text and send
        this.socket.emit('voice_message', {
            audio_data: audioBlob,
            conversation_id: this.conversationId,
            format: 'webm'
        });
    }
    
    subscribeToSystemAlerts() {
        this.socket.on('system_alert', (alert) => {
            // Show mobile notification
            this.showPushNotification(alert);
        });
    }
    
    showPushNotification(alert) {
        // Mobile notification logic
        console.log(`Alert: ${alert.message}`);
    }
}

export default UltimateAgentMobile;
```

---

## üîí Enhanced Security & Authentication

### OAuth 2.0 Integration
```http
POST /api/v4/auth/oauth/authorize
Content-Type: application/json

{
  "client_id": "mobile_app_123",
  "scope": ["chat", "inference", "p2p", "monitoring"],
  "device_info": {
    "platform": "ios",
    "version": "16.0",
    "device_id": "iPhone14,2"
  }
}
```

### API Rate Limits (Enhanced)
- **Chat API:** 500 messages per hour per user
- **P2P Inference:** 100 requests per hour per node
- **Distributed Training:** 10 concurrent tasks per account
- **WebSocket Connections:** 20 per IP address
- **File Uploads:** 50MB per file, 500MB per hour

### Security Headers
```http
X-Content-Type-Options: nosniff
X-Frame-Options: DENY
X-XSS-Protection: 1; mode=block
Strict-Transport-Security: max-age=31536000; includeSubDomains
Content-Security-Policy: default-src 'self'
X-Agent-Version: 4.1.0
X-Features-Enabled: chat,p2p,ollama,blockchain
```

---

## üìä Performance Optimization Guidelines

### Best Practices

#### Chat Performance
```javascript
// Good: Use WebSocket for real-time chat
socket.emit('chat_message', {message: 'Hello'});

// Good: Enable context caching
const chatOptions = {
    context_aware: true,
    cache_context: true,
    max_context_length: 2000
};

// Avoid: Polling for responses
// setInterval(() => checkChatResponse(), 100); // DON'T DO THIS
```

#### P2P Network Optimization
```python
# Optimal P2P configuration
p2p_config = {
    "max_peers": 10,  # Balance connectivity vs overhead
    "heartbeat_interval": 30,  # Not too frequent
    "consensus_threshold": 0.67,  # 2/3 majority
    "redundancy_factor": 3,  # Sufficient backup
    "timeout": 30.0  # Allow for network latency
}
```

#### Inference Batching
```javascript
// Good: Batch similar requests
const batchRequests = [
    {model: 'sentiment', input: 'Text 1'},
    {model: 'sentiment', input: 'Text 2'},
    {model: 'sentiment', input: 'Text 3'}
];

fetch('/api/v4/ai/inference/batch', {
    method: 'POST',
    body: JSON.stringify({requests: batchRequests})
});

// Avoid: Individual requests for batch operations
```

### Monitoring & Debugging

#### Enable Debug Mode
```http
POST /api/v4/config/debug
Content-Type: application/json

{
  "debug_level": "verbose",
  "components": ["ai", "p2p", "blockchain", "chat"],
  "log_performance": true,
  "trace_requests": true
}
```

#### Performance Metrics
```http
GET /api/v4/performance/detailed?component=all&timeframe=1h
```

---

## üöÄ Future Roadmap (v4.2+)

### Planned Features
- **Multi-modal AI**: Image, audio, and video processing
- **Advanced P2P**: Cross-chain interoperability
- **Edge Computing**: Mobile and IoT device integration
- **Federated Learning**: Privacy-preserving distributed training
- **Quantum-Ready**: Post-quantum cryptography support
- **VR/AR Integration**: 3D spatial AI interactions

### Experimental APIs (Preview)
```http
GET /api/v5/preview/multimodal/vision
GET /api/v5/preview/quantum/encryption
GET /api/v5/preview/edge/deployment
```

---

This comprehensive API documentation covers all current features and capabilities of the Enhanced Ultimate Pain Network Agent v4.1. For the latest updates, examples, and community support, visit our [GitHub repository](https://github.com/ultimate-agent/modular) or join our [Discord community](https://discord.gg/ultimate-agent).

**Happy Building! üöÄ**