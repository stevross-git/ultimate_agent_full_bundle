[LOCAL_AI]
enabled = true
auto_model_management = true
preload_models = true
prefer_local_ai = true
fallback_to_cloud = true
max_concurrent_requests = 3

[OLLAMA]
host = localhost
port = 11434
timeout = 30.0
auto_pull_models = true
model_selection_strategy = auto
default_model = auto

[LOCAL_AI_MODELS]
general_model = llama2:7b-q4_0
coding_model = codellama:7b-q4_0
creative_model = mistral:7b-q4_0
technical_model = llama2:13b-q4_0

[LOCAL_AI_PERFORMANCE]
enable_gpu_acceleration = auto
batch_size = 1
context_length = 2048
temperature = 0.7
top_p = 0.9
repeat_penalty = 1.1